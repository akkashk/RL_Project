{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95  # Discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99  # Closer to one the slower epsilon decays\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)[0]\n",
    "        return np.argmax(act_values)\n",
    "    \n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        We randomly choose 'batch_size' number of action-reward from start of training and update NN with it.\n",
    "        If past is dominated by negative experince, could result in very few positive examples to learn NN. Either increase \n",
    "        batch_size for this OR ensure we get a good mix of all possible reards when choosing from past experiences.\n",
    "        \"\"\"\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)[0]\n",
    "            target_f[action] = target\n",
    "            target_f = target_f.reshape(1, len(target_f))  # Output of a layer has shape (*, x), here (*, 2)\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/500, score: 1.0, e: 1.0\n",
      "episode: 1/500, score: 1.0, e: 0.99\n",
      "episode: 2/500, score: 1.0, e: 0.98\n",
      "episode: 3/500, score: 1.0, e: 0.97\n",
      "episode: 4/500, score: 1.0, e: 0.96\n",
      "episode: 5/500, score: 1.0, e: 0.95\n",
      "episode: 6/500, score: 1.0, e: 0.94\n",
      "episode: 7/500, score: 1.0, e: 0.93\n",
      "episode: 8/500, score: 1.0, e: 0.92\n",
      "episode: 9/500, score: 1.0, e: 0.91\n",
      "episode: 10/500, score: 1.0, e: 0.9\n",
      "episode: 11/500, score: 1.0, e: 0.9\n",
      "episode: 12/500, score: 1.0, e: 0.89\n",
      "episode: 13/500, score: 1.0, e: 0.88\n",
      "episode: 14/500, score: 1.0, e: 0.87\n",
      "episode: 15/500, score: 1.0, e: 0.86\n",
      "episode: 16/500, score: 1.0, e: 0.85\n",
      "episode: 17/500, score: 1.0, e: 0.84\n",
      "episode: 18/500, score: 1.0, e: 0.83\n",
      "episode: 19/500, score: 1.0, e: 0.83\n",
      "episode: 20/500, score: 1.0, e: 0.82\n",
      "episode: 21/500, score: 1.0, e: 0.81\n",
      "episode: 22/500, score: 1.0, e: 0.8\n",
      "episode: 23/500, score: 1.0, e: 0.79\n",
      "episode: 24/500, score: 1.0, e: 0.79\n",
      "episode: 25/500, score: 1.0, e: 0.78\n",
      "episode: 26/500, score: 1.0, e: 0.77\n",
      "episode: 27/500, score: 1.0, e: 0.76\n",
      "episode: 28/500, score: 1.0, e: 0.75\n",
      "episode: 29/500, score: 1.0, e: 0.75\n",
      "episode: 30/500, score: 1.0, e: 0.74\n",
      "episode: 31/500, score: 1.0, e: 0.73\n",
      "episode: 32/500, score: 1.0, e: 0.72\n",
      "episode: 33/500, score: 1.0, e: 0.72\n",
      "episode: 34/500, score: 1.0, e: 0.71\n",
      "episode: 35/500, score: 1.0, e: 0.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4f152454c707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-f6ec5969f59e>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Output of a layer has shape (*, x), here (*, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/Python-3.5.1/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/python/Python-3.5.1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/python/Python-3.5.1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/Python-3.5.1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/Python-3.5.1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/Python-3.5.1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/Python-3.5.1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/Python-3.5.1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/Python-3.5.1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPISODES = 500\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "env = gym.make(ENV_NAME)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "batch_size = 64\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(1000):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward # if not done else -10\n",
    "        next_state = np.reshape(next_state, (1, state_size))\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, EPISODES, reward, agent.epsilon))\n",
    "    \n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 16.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "# Use the trained agent (DQN) to play a game\n",
    "state = env.reset()\n",
    "# state = np.reshape(state, [1, state_size])\n",
    "totalreward = 0\n",
    "frames = []\n",
    "for _ in range(500):  # Each episode is run for 100 timesteps\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    action = env.action_space.sample() # agent.act(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "#     state = np.reshape(next_state, [1, state_size])\n",
    "    totalreward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "frames.append(env.render(mode='rgb_array'))\n",
    "print(\"Total reward:\", totalreward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5190063e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the game\n",
    "img = plt.imshow(frames[0]) # only call this once\n",
    "for f in frames[1:]:\n",
    "    img.set_data(f)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
