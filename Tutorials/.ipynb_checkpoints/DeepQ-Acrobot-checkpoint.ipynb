{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory_pos = []\n",
    "        self.memory_neg = []\n",
    "        self.gamma = 0.95  # Discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99  # Closer to one the slower epsilon decays\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # For Atari gym, positive rewards good and negative rewards bad\n",
    "        if reward < 0:\n",
    "            self.memory_neg.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.memory_pos.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)[0]\n",
    "        return np.argmax(act_values)\n",
    "    \n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        We randomly choose 'batch_size' number of action-reward from start of training and update NN with it.\n",
    "        If past is dominated by negative experince, could result in very few positive examples to learn NN. Either increase \n",
    "        batch_size for this OR ensure we get a good mix of all possible reards when choosing from past experiences.\n",
    "        \"\"\"\n",
    "        batch_size = min(batch_size, len(self.memory_neg))  # random.sample throws error is batch_size > length of list\n",
    "        mini_batch = random.sample(self.memory_neg, batch_size)  # Samples with bad reward\n",
    "        \n",
    "        batch_size = min(15, len(self.memory_pos))\n",
    "        mini_batch.extend(random.sample(self.memory_pos, batch_size))  # Samples with good reward\n",
    "        \n",
    "        xs = []\n",
    "        ys = []\n",
    "        \n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)[0]\n",
    "            target_f[action] = target\n",
    "            target_f = target_f.reshape(1, len(target_f))  # Output of a layer has shape (*, x), here (*, 2)\n",
    "#             self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            xs.append(state.squeeze())\n",
    "            ys.append(target_f.squeeze())\n",
    "        \n",
    "        xs = np.array(xs)\n",
    "        ys = np.array(ys)\n",
    "        self.model.fit(xs, ys, epochs=1, verbose=0, batch_size=47)  # Change batch_size to see how training changes, lower seems to be better here\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 200\n",
    "\n",
    "ENV_NAME = 'Acrobot-v1'\n",
    "env = gym.make(ENV_NAME)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(1000):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward # if not done else -10\n",
    "        next_state = np.reshape(next_state, (1, state_size))\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, EPISODES, reward, agent.epsilon))\n",
    "    \n",
    "    agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -500.0\n"
     ]
    }
   ],
   "source": [
    "# Use the trained agent (DQN) to play a game\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "totalreward = 0\n",
    "frames = []\n",
    "for _ in range(1000):  # Each episode is run for 100 timesteps\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    state = np.reshape(next_state, [1, state_size])\n",
    "    totalreward += reward\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "frames.append(env.render(mode='rgb_array'))\n",
    "print(\"Total reward:\", totalreward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcd872035c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the game\n",
    "img = plt.imshow(frames[0]) # only call this once\n",
    "for f in frames[-25:]:  #[-25:]:  If you only need to look at the last few seconds or [1:]: for the entire video\n",
    "    img.set_data(f)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
