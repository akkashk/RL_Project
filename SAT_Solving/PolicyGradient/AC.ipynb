{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from queue import Queue\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from random import shuffle\n",
    "from itertools import combinations\n",
    "from scipy.special import comb\n",
    "\n",
    "from matplotlib2tikz import save as tikz_save\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_literal = lambda x: x[1:] if x.startswith('-') else '-'+x\n",
    "deepcopy = lambda x: pickle.loads(pickle.dumps(x))\n",
    "\n",
    "def parse_input(input_file):\n",
    "    \n",
    "    \"\"\"\n",
    "    literal_clauseNum: {Literal: Set of clause numbers that are still in consideration for this variable}                        \n",
    "    \n",
    "    clauseNum_clause: {Clause number: Set of literals that could still satisfy this clause}\n",
    "    \n",
    "    literal_boolen: {Literal: boolean on whether literal set of True/False/None, None meaning could be either, doesn't matter}\n",
    "    \n",
    "    input_file:\n",
    "    c DIMACS CNF: conjunction/AND of one or more clauses, where a clause is a disjunction/OR of literals\n",
    "    c Comments start with a c, First lines begins with p and describes the probelm and all clause lines end with a 0\n",
    "    c Can't have same variable in both forms in same clause. So A ~A is not allowed. Can have them in separate clauses.\n",
    "                        \n",
    "    \"\"\"\n",
    "\n",
    "    all_clauses = []  # List of all clauses that appear in input. Used for SAT checking the mapping given by DPLL\n",
    "\n",
    "    literal_clauseNum = defaultdict(set)\n",
    "\n",
    "    def filler():\n",
    "        return None\n",
    "\n",
    "    literal_boolen = defaultdict(filler)\n",
    "\n",
    "    clauseNum_clause = {}\n",
    "\n",
    "    clause_counter = 0\n",
    "\n",
    "    with open(input_file, 'r') as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            # Do checks on comments\n",
    "            if line.startswith('c') or line.startswith('p') or line.startswith('0') or line.startswith('%'):\n",
    "                continue\n",
    "            if len(line) > 0:\n",
    "                clause = []\n",
    "                clause_counter += 1\n",
    "                for literal in line.split():\n",
    "                    if literal == '0':\n",
    "                        # End of line, ignore in DIMACS CNF format\n",
    "                        continue\n",
    "                    clause.append(literal)\n",
    "                    literal_clauseNum[literal].add(clause_counter)\n",
    "                clauseNum_clause[clause_counter] = set(clause)\n",
    "                all_clauses.append(clause)\n",
    "\n",
    "    return literal_clauseNum, clauseNum_clause, literal_boolen, all_clauses\n",
    "\n",
    "def unit_prop(literal_clauseNum, clauseNum_clause, literal_boolen):\n",
    "    keep_updating = True\n",
    "    while keep_updating:\n",
    "        keep_updating = False # Assuming we've found all unit clauses\n",
    "        for clauseNum in list(clauseNum_clause.keys()):\n",
    "            if clauseNum not in clauseNum_clause:\n",
    "                continue\n",
    "            clause = clauseNum_clause[clauseNum]\n",
    "            # Clause contains the remaining literals that could potentially satisfy this clause. \n",
    "            if len(clause) == 0:\n",
    "                # Empty clause, so need to return True for empty clause detected\n",
    "                return True, None, None, None\n",
    "            if len(clause) > 1:\n",
    "                # Can't do unit prop \n",
    "                continue\n",
    "\n",
    "            literal = clause.pop()  # Needs to be set to True\n",
    "            clause.add(literal)  # Removed later\n",
    "            literal_boolen[literal] = True\n",
    "            keep_updating = True  # Since we found one unit clause, maybe more\n",
    "\n",
    "    #         print(literal)\n",
    "    #         print(literal_clauseNum)\n",
    "    #         print(clauseNum_clause)\n",
    "\n",
    "            # For all clauses that have this literal, they have been satisfied now\n",
    "            # 1) Gather all pairs of (literals, clauseNum) that appear in these clauses so we can remove them from literal_clauseNum\n",
    "            # 2) Delete these clauses from clauseNum_clause\n",
    "            pairs_to_delete = []\n",
    "            for clauseNums_with_literal in literal_clauseNum[literal]:\n",
    "                for literals_in_clauseNums in clauseNum_clause[clauseNums_with_literal]:\n",
    "                    pairs_to_delete.append((literals_in_clauseNums, clauseNums_with_literal))\n",
    "\n",
    "    #         print(pairs_to_delete)\n",
    "\n",
    "            for literals_in_clauseNums, clauseNums_with_literal in pairs_to_delete:\n",
    "                literal_clauseNum[literals_in_clauseNums].discard(clauseNums_with_literal)\n",
    "                if clauseNums_with_literal in clauseNum_clause:\n",
    "                    del clauseNum_clause[clauseNums_with_literal]\n",
    "\n",
    "            # For all the clauses with opposite literal, remove the literal from the clause\n",
    "            if switch_literal(literal) not in literal_clauseNum: # if the opposite variable doesn't exist, skip\n",
    "                continue\n",
    "\n",
    "            opposite_literal = switch_literal(literal)\n",
    "            literal_boolen[opposite_literal] = False\n",
    "\n",
    "            for clauseNums_with_literal in literal_clauseNum[opposite_literal]:\n",
    "                clauseNum_clause[clauseNums_with_literal].discard(opposite_literal)\n",
    "\n",
    "            literal_clauseNum[opposite_literal] = set()  # It is not watching any clauses anymore\n",
    "\n",
    "    #         print(\"OPPO\")\n",
    "    #         print(literal_clauseNum)\n",
    "    #         print(clauseNum_clause)\n",
    "        \n",
    "    return False, literal_clauseNum, clauseNum_clause, literal_boolen\n",
    "\n",
    "\n",
    "def pure_literal(literal_clauseNum, clauseNum_clause, literal_boolen):\n",
    "    keep_updating = True\n",
    "    while keep_updating:\n",
    "        keep_updating = False\n",
    "        for literal in list(literal_clauseNum.keys()):\n",
    "            if literal in literal_boolen:\n",
    "                continue\n",
    "\n",
    "            opposite_literal = switch_literal(literal)\n",
    "            if opposite_literal not in literal_boolen: # The opposite variable has not been assigned yet\n",
    "                # If it doesn't exist or it does but it doesn't have to satisfy any clauses\n",
    "                if opposite_literal not in literal_clauseNum or len(literal_clauseNum[opposite_literal]) == 0:\n",
    "                    # LITERAL IS A PURE LITERAL\n",
    "                    keep_updating = True\n",
    "                    literal_boolen[literal] = True\n",
    "\n",
    "                    # All the clauses that literal exists in has been made true, so remove the clauses and make literal watch no clause\n",
    "                    pairs_to_delete = []\n",
    "                    for clauseNums_with_literal in literal_clauseNum[literal]:\n",
    "                        for literals_in_clauseNums in clauseNum_clause[clauseNums_with_literal]:\n",
    "                            pairs_to_delete.append((literals_in_clauseNums, clauseNums_with_literal))\n",
    "\n",
    "            #         print(pairs_to_delete)\n",
    "\n",
    "                    for literals_in_clauseNums, clauseNums_with_literal in pairs_to_delete:\n",
    "                        literal_clauseNum[literals_in_clauseNums].discard(clauseNums_with_literal)\n",
    "                        if clauseNums_with_literal in clauseNum_clause:\n",
    "                            del clauseNum_clause[clauseNums_with_literal]\n",
    "                        \n",
    "    return literal_clauseNum, clauseNum_clause, literal_boolen\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_var(literal, boolean, literal_clauseNum, clauseNum_clause, literal_boolen):\n",
    "    literal_boolen[literal] = boolean\n",
    "\n",
    "    if boolean == False:\n",
    "        literal = switch_literal(literal)\n",
    "        literal_boolen[literal] = True\n",
    "    \n",
    "    # Unit-prop logic below\n",
    "    pairs_to_delete = []\n",
    "    for clauseNums_with_literal in literal_clauseNum[literal]:\n",
    "        for literals_in_clauseNums in clauseNum_clause[clauseNums_with_literal]:\n",
    "            pairs_to_delete.append((literals_in_clauseNums, clauseNums_with_literal))\n",
    "\n",
    "    #         print(pairs_to_delete)\n",
    "\n",
    "    for literals_in_clauseNums, clauseNums_with_literal in pairs_to_delete:\n",
    "        literal_clauseNum[literals_in_clauseNums].discard(clauseNums_with_literal)\n",
    "        if clauseNums_with_literal in clauseNum_clause:\n",
    "            del clauseNum_clause[clauseNums_with_literal]\n",
    "\n",
    "    # For all the clauses with opposite literal, remove the literal from the clause\n",
    "    if switch_literal(literal) not in literal_clauseNum: # if the opposite variable doesn't exist, skip\n",
    "        return literal_clauseNum, clauseNum_clause, literal_boolen\n",
    "\n",
    "    opposite_literal = switch_literal(literal)\n",
    "    literal_boolen[opposite_literal] = False\n",
    "\n",
    "    for clauseNums_with_literal in literal_clauseNum[opposite_literal]:\n",
    "        clauseNum_clause[clauseNums_with_literal].discard(opposite_literal)\n",
    "\n",
    "    literal_clauseNum[opposite_literal] = set()  # It is not watching any clauses anymore\n",
    "\n",
    "    #         print(\"OPPO\")\n",
    "    #         print(literal_clauseNum)\n",
    "    #         print(clauseNum_clause)\n",
    "    \n",
    "    return literal_clauseNum, clauseNum_clause, literal_boolen\n",
    "\n",
    "\n",
    "def choose_var(literal_clauseNum, clauseNum_clause, literal_boolen, algo='maxo'):\n",
    "    # Choosing the first literal every time\n",
    "#     remaining_clauses = list(clauseNum_clause.values())\n",
    "#     literal = remaining_clauses[0].pop()\n",
    "#     remaining_clauses[0].add(literal)\n",
    "\n",
    "    # Using heuristics\n",
    "    if algo == 'maxo':\n",
    "        literal, _ = maxo(literal_clauseNum)\n",
    "    elif algo == 'moms':\n",
    "        literal, _ = moms(literal_clauseNum, clauseNum_clause)\n",
    "    elif algo == 'mams':\n",
    "        literal, _ = mams(literal_clauseNum, clauseNum_clause)\n",
    "    elif algo == 'jw':\n",
    "        literal, _ = jw(literal_clauseNum, clauseNum_clause)\n",
    "    elif algo == 'jw_2':\n",
    "        literal, _ = jw_2(literal_clauseNum, clauseNum_clause)\n",
    "    elif algo == 'bohm':\n",
    "        literal, _ = bohm(literal_clauseNum, clauseNum_clause)\n",
    "    \n",
    "\n",
    "    return literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxo(literal_clauseNum, return_counts=False):\n",
    "    \"\"\"\n",
    "    Returns the literal that appears in the most number of clauses\n",
    "    \"\"\"\n",
    "    literal_count = defaultdict(int)\n",
    "    for literal, clauseNums in literal_clauseNum.items():\n",
    "        literal_count[literal] = len(clauseNums)\n",
    "    if return_counts:\n",
    "        return literal_count\n",
    "    \n",
    "    max_lit = None\n",
    "    max_count = 0\n",
    "    for literal, count in literal_count.items():\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            max_lit = literal\n",
    "    return max_lit, max_count\n",
    "\n",
    "\n",
    "def moms(literal_clauseNum, clauseNum_clause, return_counts=False):\n",
    "    \"\"\"\n",
    "    Returns the literal that appears most in the smallest clauses\n",
    "    \"\"\"\n",
    "    # Select the clausesNumbers for clauses of the smaller size\n",
    "    least_size = min(map(len, clauseNum_clause.values()))\n",
    "    literal_count = defaultdict(int)\n",
    "    \n",
    "    for literal, clauseNums in literal_clauseNum.items():\n",
    "        for clauseNum in clauseNums:\n",
    "            if len(clauseNum_clause[clauseNum]) == least_size:\n",
    "                # Each time a literal appears in a least-size clause we \n",
    "                # increment counter by 1\n",
    "                literal_count[literal] += 1\n",
    "    \n",
    "    if return_counts:\n",
    "        return literal_count\n",
    "    \n",
    "    max_lit = None\n",
    "    max_count = 0\n",
    "    for literal, count in literal_count.items():\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            max_lit = literal\n",
    "    return max_lit, max_count\n",
    "\n",
    "\n",
    "def mams(literal_clauseNum, clauseNum_clause):\n",
    "    \"\"\"\n",
    "    Combines MAXO and MOMS count statistics from above and chooses the literal that appears most between them\n",
    "    \"\"\"\n",
    "    maxo_ans = maxo(literal_clauseNum, return_counts=True)\n",
    "    moms_ans = moms(literal_clauseNum, clauseNum_clause, return_counts=True)\n",
    "    \n",
    "    # MAXO would return the dict with most keys\n",
    "    for literal in maxo_ans:\n",
    "        maxo_ans[literal] += moms_ans[literal]\n",
    "        # Since using defaultdict we add 0 if literal not in moms_ans\n",
    "    \n",
    "    max_lit = None\n",
    "    max_count = 0\n",
    "    for literal, count in maxo_ans.items():\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            max_lit = literal\n",
    "    \n",
    "    return max_lit, max_count\n",
    "\n",
    "\n",
    "def jw(literal_clauseNum, clauseNum_clause):\n",
    "    \"\"\"\n",
    "    Jeroslow-Wang Rule\n",
    "    \"\"\"\n",
    "    literal_score = defaultdict(int)\n",
    "    \n",
    "    for literal, clauseNums in literal_clauseNum.items():\n",
    "        for clauseNum in clauseNums:\n",
    "            clause = clauseNum_clause[clauseNum]\n",
    "            literal_score[literal] += 2 ** -len(clause)\n",
    "    \n",
    "    max_lit = None\n",
    "    max_score = 0\n",
    "    for literal, score in literal_score.items():\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            max_lit = literal\n",
    "            \n",
    "    return max_lit, max_score\n",
    "\n",
    "def jw_2(literal_clauseNum, clauseNum_clause):\n",
    "    \"\"\"\n",
    "    2-sided JW rule. See Heutistics folder\n",
    "    \"\"\"\n",
    "    \n",
    "    literal_score = defaultdict(int)\n",
    "    \n",
    "    for literal, clauseNums in literal_clauseNum.items():\n",
    "        for clauseNum in clauseNums:\n",
    "            clause = clauseNum_clause[clauseNum]\n",
    "            literal_score[literal] += 2 ** -len(clause)\n",
    "    \n",
    "    max_lit = None\n",
    "    max_score = 0\n",
    "    for literal, score in list(literal_score.items()):\n",
    "        other_literal = switch_literal(literal)\n",
    "        total_score = score + literal_score[other_literal]\n",
    "        \n",
    "        if total_score > max_score:\n",
    "            max_score = score\n",
    "            max_lit = literal if score >= literal_score[other_literal] else other_literal\n",
    "            \n",
    "    return max_lit, max_score\n",
    "\n",
    "\n",
    "def bohm(literal_clauseNum, clauseNum_clause):\n",
    "    \"\"\"\n",
    "    See Heuristics folder. Lexicographic order of the vector (H1(x), H2(x), ..., Hn(x)) means we first choose highest H1(x)\n",
    "    variable. When tied we then choose amongst tied variable highest H2 variable. When tied then H3 and so on.\n",
    "    \n",
    "    We've had to manage edge cases here but don't mention that in report. Only give formula from paper\n",
    "    \"\"\"\n",
    "    pos_literal_count = defaultdict(lambda: [0, 0, 0])  # This default initialisation only works for 3 SAT\n",
    "    neg_literal_count = defaultdict(lambda: [0, 0, 0])\n",
    "    \n",
    "    for literal, clauseNums in literal_clauseNum.items():\n",
    "        if literal.startswith('-'):\n",
    "            for clauseNum in clauseNums:\n",
    "                clause = clauseNum_clause[clauseNum]\n",
    "                neg_literal_count[literal][len(clause)-1] += 1\n",
    "        else:\n",
    "            for clauseNum in clauseNums:\n",
    "                clause = clauseNum_clause[clauseNum]\n",
    "                pos_literal_count[literal][len(clause)-1] += 1\n",
    "                \n",
    "    final_count = []\n",
    "    # Sometimes we only have negative literals left. So then we just use those\n",
    "    for literal, pos_counts in (pos_literal_count.items() or neg_literal_count.items()):\n",
    "        other_literal = switch_literal(literal)\n",
    "        \n",
    "        if literal.startswith('-'):\n",
    "            # pos_literal_counts is empty. So literal and pos_counts actually are neg_literal_counts\n",
    "            neg_counts = pos_literal_count[other_literal]\n",
    "        else:\n",
    "            # pos_literal_counts isn't empty. So continue as normal\n",
    "            neg_counts = neg_literal_count[other_literal]\n",
    "        \n",
    "        final_count.append(([max(p, n) + 2 * min(p, n) for p, n in zip(pos_counts, neg_counts)], literal))\n",
    "            \n",
    "    final_count.sort(reverse=True)\n",
    "    score_vector, literal = final_count[0]\n",
    "    other_literal = switch_literal(literal)\n",
    "    \n",
    "    if literal.startswith('-'):\n",
    "        neg_literal = literal\n",
    "        pos_literal = other_literal\n",
    "    else:\n",
    "        neg_literal = other_literal\n",
    "        pos_literal = literal\n",
    "    \n",
    "    # Since the score for positive and negative literal is the same, choose one which the highest overall score\n",
    "    if sum(pos_literal_count[pos_literal]) >= sum(neg_literal_count[neg_literal]):\n",
    "        literal = pos_literal\n",
    "    else:\n",
    "        literal = neg_literal\n",
    "    \n",
    "    return literal, score_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods that are used by the Env class to get state features\n",
    "\n",
    "def number_of_variables(literal_clauseNum):\n",
    "    \"\"\" Returns the number of total variables (including repeats) present in the remaining clauses \"\"\"\n",
    "    return sum(map(len, literal_clauseNum.values()))\n",
    "\n",
    "\n",
    "def horn_clause_ratio(clauseNum_clause):\n",
    "    \"\"\" Returns the ratio of horn clauses to total number of clauses \"\"\"\n",
    "    horn_count = 0\n",
    "    total_count = 0\n",
    "    for clause in clauseNum_clause.values():\n",
    "        if len(clause) > 0:\n",
    "            total_count += 1\n",
    "        if len(list(filter(lambda x: not x.startswith('-'), clause))) == 1:\n",
    "            horn_count += 1\n",
    "\n",
    "    return horn_count / total_count if total_count > 0 else 0\n",
    "\n",
    "\n",
    "def horn_clause_count(literal_clauseNum, clauseNum_clause):\n",
    "    \"\"\"\n",
    "    For each variable, we count the number of Horn clauses it is present in\n",
    "    \"\"\"\n",
    "    literal_count = defaultdict(int)\n",
    "    for literal, clauseNums in literal_clauseNum.items():\n",
    "        for clauseNum in clauseNums:\n",
    "            clause = clauseNum_clause[clauseNum]\n",
    "            if len(list(filter(lambda x: not x.startswith('-'), clause))) == 1:\n",
    "                literal_count[literal] += 1\n",
    "                \n",
    "    counts = list(literal_count.values())\n",
    "    return np.array(counts) if len(counts) > 0 else np.array([0])\n",
    "\n",
    "\n",
    "def clause_to_variable_ratio(literal_clauseNum):\n",
    "    \"\"\" Returns the clause to variable ratio: c/v which predict problem hardness \"\"\"\n",
    "    clauses = set()\n",
    "    num_literals = 0\n",
    "    for literal, clauseNums in literal_clauseNum.items():\n",
    "        if len(clauseNums) > 0:\n",
    "            clauses = clauses.union(clauseNums)\n",
    "            if literal.startswith('-'):\n",
    "                num_literals += 1\n",
    "            else:\n",
    "                num_literals += 1\n",
    "\n",
    "    return 0 if num_literals == 0 else len(clauses) / num_literals\n",
    "\n",
    "\n",
    "def pos_neg_ratio(literal_clauseNum):\n",
    "    \"\"\"\n",
    "    Returns the number of positive literals (incl repeats) to negative literals (incl repeats) in the clauses.\n",
    "    THIS DOESN'T GIVE USEFUL STATE INFORMATION WHEN USED ALONE\n",
    "    \"\"\"\n",
    "    pos_literal_count = 0\n",
    "    neg_literal_count = 0\n",
    "\n",
    "    for literal, clauseNums in literal_clauseNum.items():\n",
    "        if literal.startswith('-'):\n",
    "            neg_literal_count += len(clauseNums)\n",
    "        else:\n",
    "            pos_literal_count += len(clauseNums)\n",
    "\n",
    "    return pos_literal_count / neg_literal_count if neg_literal_count > 0 else pos_literal_count\n",
    "\n",
    "def pos_neg_ratio_per_var():\n",
    "    pass\n",
    "\n",
    "\n",
    "def CVIG(literal_clauseNum, clauseNum_clause):\n",
    "    \"\"\"\n",
    "    Caluse-variable incidence graph. We create a bipartite graph (a matrix) with literals in columns and clauses in rows.\n",
    "    See Features_2 PDF file.\n",
    "    \"\"\"\n",
    "    if len(clauseNum_clause) == 0:\n",
    "        return 0\n",
    "    \n",
    "    literal_index_mapping = {}\n",
    "    clauseNum_index_mapping = {}\n",
    "    \n",
    "    for i, literal in enumerate(literal_clauseNum.keys()):\n",
    "        literal_index_mapping[literal] = i\n",
    "        \n",
    "    for i, clauseNum in enumerate(clauseNum_clause):\n",
    "        clauseNum_index_mapping[clauseNum] = i\n",
    "    \n",
    "    graph = np.zeros((len(literal_index_mapping), len(clauseNum_index_mapping)))\n",
    "    for literal, clauseNums in literal_clauseNum.items():\n",
    "        for clauseNum in clauseNums:\n",
    "            graph[literal_index_mapping[literal]] [clauseNum_index_mapping[clauseNum]] = 1/len(clauseNums)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def VIG(literal_clauseNum, clauseNum_clause):\n",
    "    \"\"\"\n",
    "    Variable incidence graph.\n",
    "    \"\"\"\n",
    "    if len(clauseNum_clause) == 0:\n",
    "        return 0\n",
    "    \n",
    "    literal_index_mapping = {}\n",
    "    \n",
    "    for i, literal in enumerate(literal_clauseNum.keys()):\n",
    "        literal_index_mapping[literal] = i\n",
    "        \n",
    "    graph = np.zeros((len(literal_index_mapping), len(literal_index_mapping)))\n",
    "    \n",
    "    for clause in clauseNum_clause.values():\n",
    "        if len(clause) < 2:\n",
    "            continue\n",
    "        for x, y in combinations(clause, 2):\n",
    "            w = 1 / (comb(len(clause), 2))  # Try combinations with replacement to add self-loops\n",
    "            graph[literal_index_mapping[x]][literal_index_mapping[y]] = w\n",
    "            graph[literal_index_mapping[y]][literal_index_mapping[x]] = w\n",
    "            \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    \n",
    "    def __init__(self, input_file):\n",
    "        self.input_file = input_file\n",
    "        self.stack = [] # We use a stack to hold the next states to explore. i.e. we do DFS as less memory requirements than BFS\n",
    "        self.state = None\n",
    "        self.actions = {0: 'maxo', 1: 'moms', 2: 'mams', 3: 'jw', 4: 'jw_2', 5: 'bohm'}\n",
    "        self.action_penalty = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}  # Penalty to give each action\n",
    "    \n",
    "    def reset(self):\n",
    "        # Returns state\n",
    "        literal_clauseNum, clauseNum_clause, literal_boolen, _ = parse_input(self.input_file)\n",
    "        self.state = (literal_clauseNum, clauseNum_clause, literal_boolen)\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        literal_clauseNum, clauseNum_clause, literal_boolen = self.state\n",
    "        \n",
    "        num_var = number_of_variables(literal_clauseNum)\n",
    "        horn_clause = horn_clause_ratio(clauseNum_clause)\n",
    "\n",
    "#         var_horn_counts = horn_clause_count(literal_clauseNum, clauseNum_clause)\n",
    "#         var_horn_mean, var_horn_var = np.mean(var_horn_counts), np.var(var_horn_counts)  # DON'T USE\n",
    "        \n",
    "        pn_ratio = pos_neg_ratio(literal_clauseNum)\n",
    "#         c_v_ratio = clause_to_variable_ratio(literal_clauseNum)\n",
    "        \n",
    "        cvig_graph = CVIG(literal_clauseNum, clauseNum_clause)\n",
    "        cvig_mean, cvig_var = np.mean(cvig_graph), np.var(cvig_graph)  # axis=0 gives more different results if we want this to return vector\n",
    "        \n",
    "#         vig_graph = VIG(literal_clauseNum, clauseNum_clause)\n",
    "#         vig_mean, vig_var = np.mean(vig_graph), np.var(vig_graph)\n",
    "        \n",
    "#         return [num_var, c_v_ratio, cvig_mean, cvig_var]\n",
    "        return [num_var, horn_clause, pn_ratio, cvig_var]\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns: next_state_1, next_state_2, reward, done\n",
    "        reward = 0 if reached a leaf node, 0 if not\n",
    "        \"\"\"\n",
    "        literal_clauseNum, clauseNum_clause, literal_boolen = self.state\n",
    "        \n",
    "        num_clauses_start = 0\n",
    "        for clause in clauseNum_clause.values():\n",
    "            if len(clause) > 0:\n",
    "                num_clauses_start += 1\n",
    "            else:\n",
    "                # This is reached when we reached an UNSAT state in previous step and popped off another UNSAT from the stack\n",
    "                isEmpty = len(self.stack) == 0\n",
    "                if not isEmpty:\n",
    "                    self.state = self.stack.pop()\n",
    "                return None, -1 + self.action_penalty[action], isEmpty\n",
    "        \n",
    "        literal = choose_var(literal_clauseNum, clauseNum_clause, literal_boolen, algo=self.actions[action])\n",
    "        \n",
    "        # Set the chosen literal to be True\n",
    "        literal_clauseNum_T, clauseNum_clause_T, literal_boolen_T = \\\n",
    "            set_var(literal, True, deepcopy(literal_clauseNum), deepcopy(clauseNum_clause), dict(literal_boolen))\n",
    "            \n",
    "        # Set new state\n",
    "        self.state = (literal_clauseNum_T, clauseNum_clause_T, literal_boolen_T)\n",
    "        \n",
    "        # Set the chosen literal to be False\n",
    "        literal_clauseNum_F, clauseNum_clause_F, literal_boolen_F = \\\n",
    "            set_var(literal, False, literal_clauseNum, clauseNum_clause, literal_boolen)\n",
    "            \n",
    "        # Add new state to queue\n",
    "        self.stack.append((literal_clauseNum_F, clauseNum_clause_F, literal_boolen_F))\n",
    "        \n",
    "        \n",
    "        if clauseNum_clause_T == {} or clauseNum_clause_F == {}:  # We have satisfied\n",
    "#             print(\"INITIAL\")\n",
    "            return None, 1 + self.action_penalty[action], True\n",
    "        \n",
    "        \n",
    "        literal_clauseNum, clauseNum_clause, literal_boolen = self.state\n",
    "        \n",
    "        # Do unit prop\n",
    "        empty_clause, literal_clauseNum, clauseNum_clause, literal_boolen = \\\n",
    "            unit_prop(literal_clauseNum, clauseNum_clause, literal_boolen)\n",
    "            \n",
    "        if empty_clause:\n",
    "            isEmpty = len(self.stack) == 0\n",
    "            if not isEmpty:\n",
    "                self.state = self.stack.pop()\n",
    "            return None, -1 + self.action_penalty[action], isEmpty\n",
    "        \n",
    "        if clauseNum_clause == {}:\n",
    "#             print(\"UNIT PROP\")\n",
    "            return None, 1 + self.action_penalty[action], True\n",
    "        \n",
    "        # Do pure literal elimination\n",
    "        literal_clauseNum, clauseNum_clause, literal_boolen = \\\n",
    "            pure_literal(literal_clauseNum, clauseNum_clause, literal_boolen)\n",
    "            \n",
    "            \n",
    "        if clauseNum_clause == {}:\n",
    "#             print(\"PLE\")\n",
    "            return None, 1 + self.action_penalty[action], True\n",
    "        \n",
    "        num_clauses_end = 0\n",
    "        for clause in clauseNum_clause.values():\n",
    "            if len(clause) > 0:\n",
    "                num_clauses_end += 1\n",
    "        \n",
    "        if num_clauses_start > 0:\n",
    "            fraction_of_clauses_removed = (num_clauses_start - num_clauses_end)/num_clauses_start\n",
    "        else:\n",
    "            fraction_of_clauses_removed = 0\n",
    "        \n",
    "        self.state = (literal_clauseNum, clauseNum_clause, literal_boolen)\n",
    "                \n",
    "        return None, -1 + self.action_penalty[action] + fraction_of_clauses_removed, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, LeakyReLU, BatchNormalization\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class PolicyEstimator():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, scope=\"policy_estimator\"):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit([np.zeros(state_space)], [np.zeros(state_space)])\n",
    "        \n",
    "        self.create_model()\n",
    "        self.model = Model(inputs=self.input, outputs=self.output)\n",
    "        \n",
    "        self.action = K.placeholder(dtype=tf.int32, name='action')\n",
    "        self.target = K.placeholder(dtype=tf.float32, name='target')\n",
    "        \n",
    "        self.chosen_action_prob = K.gather(K.squeeze(self.output, axis=0), self.action)\n",
    "        policy_grad_loss = -K.log(self.chosen_action_prob) * self.target\n",
    "        \n",
    "        entropy = -K.sum(self.output * K.log(self.output))\n",
    "        \n",
    "        self.loss = policy_grad_loss + (0.01 * -entropy)  # Maximise entropy = actions equally likely. We minimise negative entropy\n",
    "        \n",
    "        self.optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.train_op = self.optimiser.minimize(self.loss)\n",
    "        \n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Create a model for the policy to predict a probability distribution over the actions.\n",
    "        \n",
    "        OUTPUT LAYER NEEDS TO HAVE SOFTMAX ACTIVATION\n",
    "        \"\"\"\n",
    "        self.input = Input(shape=(state_space,))\n",
    "        self.h1 = Dense(2)(self.input)\n",
    "        self.a = LeakyReLU()(self.h1)\n",
    "        self.output = Dense(actions, activation='softmax')(self.a)\n",
    "        \n",
    "    def predict(self, state):\n",
    "        \"\"\"\n",
    "        Cannot use polynomial state space here.\n",
    "        \"\"\"\n",
    "        state = np.array(state)\n",
    "        if len(self.input.shape) != len(state.shape):\n",
    "            state = np.expand_dims(state, 0)\n",
    "        state = self.scaler.transform(state)\n",
    "        return np.squeeze(self.model.predict(state))\n",
    "    \n",
    "\n",
    "    def update(self, state, target, action_index):\n",
    "        sess = tf.get_default_session()\n",
    "        \n",
    "        action_index = np.array(action_index)\n",
    "        target = np.array(target)\n",
    "        state = np.array(state)\n",
    "        \n",
    "        if len(self.input.shape) != len(state.shape):\n",
    "            state = np.expand_dims(state, 0)\n",
    "        if len(action_index.shape) != 2:\n",
    "            action_index = np.expand_dims(action_index, 0)\n",
    "        if len(target.shape) != 2:\n",
    "            target = np.expand_dims(target, 0)\n",
    "            \n",
    "        self.scaler.partial_fit(state)\n",
    "        state = self.scaler.transform(state)\n",
    "        \n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict={self.input: state, self.action: action_index, self.target: target})\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEstimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. \n",
    "    \n",
    "    Only one output. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, scope=\"value_estimator\"):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit([np.zeros(state_space)], [np.zeros(state_space)])\n",
    "        \n",
    "        self.create_model()\n",
    "        \n",
    "        self.model = Model(inputs=self.input, outputs=self.output)\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=Adam(lr=learning_rate))\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        self.input = Input(shape=(state_space,))\n",
    "        self.h1 = Dense(2)(self.input)\n",
    "        self.a = LeakyReLU()(self.h1)\n",
    "        self.output = Dense(1, activation=None, kernel_initializer='zeros')(self.a)\n",
    "        \n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        state = np.array(state)\n",
    "        if len(self.input.shape) != len(state.shape):\n",
    "            state = np.expand_dims(state, 0)\n",
    "        state = self.scaler.transform(state)\n",
    "        \n",
    "        return np.squeeze(self.model.predict(state))\n",
    "\n",
    "    def update(self, state, target, sess=None):\n",
    "        target = np.array(target)\n",
    "        state = np.array(state)\n",
    "        \n",
    "        if len(state.shape) < 2:\n",
    "            state = np.expand_dims(state, 0)\n",
    "        if len(target.shape) < 2:\n",
    "            target = np.expand_dims(target, 0)\n",
    "            \n",
    "        self.scaler.partial_fit(state)\n",
    "        state = self.scaler.transform(state)\n",
    "        \n",
    "        loss = self.model.train_on_batch(state, target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_files, ϵ=1.1, estimator=None):\n",
    "\n",
    "    total_reward, total_length, total_states, total_actions = 0, 0, [], []\n",
    "    \n",
    "    if estimator is None:\n",
    "        estimator = PolicyEstimator()\n",
    "        \n",
    "    for i, filepath in enumerate(test_files):\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Testing file\", i)\n",
    "        \n",
    "        env = Env(filepath)\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action_probs = estimator.predict(state)\n",
    "            action =  np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            _, reward, done = env.step(action)\n",
    "\n",
    "            # Stats\n",
    "            total_length += 1\n",
    "            total_reward += reward\n",
    "            total_actions.append(action)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = env.get_state()\n",
    "\n",
    "    return total_reward/len(test_files), total_length/len(test_files), np.array(total_actions) #, total_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_value(training_files, batch_size=256, estimator_value=None):\n",
    "    \n",
    "    if estimator_value is None:\n",
    "        estimator_value = ValueEstimator()\n",
    "    \n",
    "    replay_memory = []\n",
    "    \n",
    "    for i, filepath in enumerate(training_files[:1000]):\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Populating', i)\n",
    "        \n",
    "        env = Env(filepath)\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action =  np.random.choice(np.arange(actions))  # Randomly choose actions\n",
    "            _, reward, done = env.step(action)\n",
    "            next_state = env.get_state()\n",
    "            replay_memory.append((state, action, reward, next_state, done))\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "    states_batch, action_batch, targets_batch = [], [], []\n",
    "    curr_episode_rewards = []\n",
    "    for state, action_idx, reward, next_state, done in replay_memory:\n",
    "        states_batch.append(state)\n",
    "        action_batch.append(action_idx)\n",
    "        curr_episode_rewards.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            # Calculate the targets from the rewards seen in episode\n",
    "            ans = list(np.cumsum(curr_episode_rewards[::-1])[::-1])  # Only works since discount factor = 1.0\n",
    "            targets_batch.extend(ans)\n",
    "            curr_episode_rewards = []\n",
    "            \n",
    "    i = 0\n",
    "    old_loss = 0\n",
    "    new_loss = 5000\n",
    "    while abs(new_loss - old_loss) > 0.01:\n",
    "        i += 1\n",
    "        states_batch, action_batch, targets_batch = np.array(states_batch), np.array(action_batch), np.array(targets_batch)      \n",
    "        # Sample some of the data points. Better than giving it new data every time\n",
    "        sample_idx = np.array(random.sample(range(len(states_batch)), batch_size))\n",
    "        if i % 1000 == 0:\n",
    "            print(new_loss, states_batch[sample_idx[0]], action_batch[sample_idx[0]], targets_batch[sample_idx[0]])\n",
    "        states_batch = states_batch[sample_idx]\n",
    "        targets_batch = targets_batch[sample_idx]\n",
    "        \n",
    "        if len(targets_batch.shape) != 2 or targets_batch.shape[1] != 1:\n",
    "            targets_batch = targets_batch.squeeze()\n",
    "            targets_batch = np.expand_dims(targets_batch, 1)\n",
    "        \n",
    "        old_loss = new_loss\n",
    "        new_loss = estimator_value.update(states_batch, targets_batch)\n",
    "    \n",
    "    print(\"Final loss:\", new_loss)\n",
    "    return estimator_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_value(training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(training_files, discount_factor=1.0, output_stats_every=1000):\n",
    "\n",
    "    estimator_policy = PolicyEstimator()\n",
    "    estimator_value = ValueEstimator()\n",
    "    \n",
    "    total_rewards = []\n",
    "    total_lengths = []\n",
    "    total_losses = []\n",
    "    value_next_predictions = [0]\n",
    "    \n",
    "    curr_reward = 0\n",
    "    curr_length = 0\n",
    "    curr_policy_loss = 0\n",
    "    curr_value_loss = 0\n",
    "    \n",
    "    estimator_value = train_value(training_files, estimator_value=estimator_value)\n",
    "    \n",
    "    for i, filepath in enumerate(training_files[1000:]):\n",
    "        \n",
    "        if i % output_stats_every == 0:\n",
    "            print(i, curr_reward / output_stats_every, curr_length / output_stats_every, curr_policy_loss / output_stats_every, curr_value_loss / output_stats_every)\n",
    "            print(value_next_predictions[-1])\n",
    "            total_rewards.append(curr_reward / output_stats_every)\n",
    "            total_lengths.append(curr_length / output_stats_every)\n",
    "            total_losses.append(curr_policy_loss / output_stats_every)\n",
    "            \n",
    "            curr_reward = 0\n",
    "            curr_length = 0\n",
    "            curr_policy_loss = 0\n",
    "            curr_value_loss = 0\n",
    "            \n",
    "        env = Env(filepath)\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action_probs = estimator_policy.predict(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            _, reward, done = env.step(action)\n",
    "            \n",
    "            # Update state\n",
    "            curr_reward += reward\n",
    "            curr_length += 1\n",
    "            \n",
    "            if done:\n",
    "#                 estimator_value.update(state, reward)\n",
    "                curr_policy_loss += estimator_policy.update(state, reward, action)\n",
    "                break\n",
    "                \n",
    "            next_state = env.get_state()\n",
    "            # Calculate TD Target\n",
    "            value_next = estimator_value.predict(next_state)\n",
    "            td_target = reward + discount_factor * value_next\n",
    "            td_error = td_target - estimator_value.predict(state)\n",
    "            \n",
    "            value_next_predictions.append(value_next)\n",
    "            \n",
    "            # Update the value estimator\n",
    "#             curr_value_loss += estimator_value.update(state, td_target)\n",
    "            \n",
    "            # Update the policy estimator\n",
    "            # using the td error as our advantage estimate\n",
    "            curr_policy_loss += estimator_policy.update(state, td_error, action)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "    \n",
    "    return total_rewards, total_lengths, total_losses, estimator_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'init_48' type=NoOp>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files: 30000\n",
      "Number of test files: 1000\n",
      "Populating 0\n",
      "Populating 100\n",
      "Populating 200\n",
      "Populating 300\n",
      "Populating 400\n",
      "Populating 500\n",
      "Populating 600\n",
      "Populating 700\n",
      "Populating 800\n",
      "Populating 900\n",
      "161.77655 [1.93000000e+02 4.49275362e-01 8.73786408e-01 3.11618583e-03] 3 [-26.09942897]\n",
      "Final loss: 111.2958\n",
      "0 0.0 0.0 0.0 0.0\n",
      "0\n",
      "100 -17.779099932107925 19.79 [-24.021662] 0.0\n",
      "-12.231142\n",
      "200 -16.62820550138067 18.75 [-21.971191] 0.0\n",
      "-11.773955\n",
      "300 -16.734416926742405 19.09 [-22.035725] 0.0\n",
      "-8.343465\n",
      "400 -16.227689435214614 18.43 [-22.200428] 0.0\n",
      "-11.704642\n",
      "500 -16.47635184544611 18.82 [-21.539814] 0.0\n",
      "-13.357837\n",
      "600 -17.10617051021294 19.31 [-23.591913] 0.0\n",
      "-11.285953\n",
      "700 -18.071972213136547 20.01 [-25.903992] 0.0\n",
      "-12.640859\n",
      "800 -18.71503888320088 20.75 [-25.986395] 0.0\n",
      "-13.190264\n",
      "900 -21.625485765894613 23.84 [-32.999073] 0.0\n",
      "-9.436754\n",
      "1000 -16.074034690880257 18.19 [-21.353786] 0.0\n",
      "-12.697313\n",
      "1100 -16.879176730730762 19.34 [-22.213554] 0.0\n",
      "-11.39625\n",
      "1200 -20.719956384777642 22.74 [-31.184528] 0.0\n",
      "-11.884885\n",
      "1300 -18.22744493798706 20.35 [-26.17784] 0.0\n",
      "-11.825912\n",
      "1400 -19.10112278664193 21.27 [-28.134336] 0.0\n",
      "-11.3772135\n",
      "1500 -18.050442587246362 20.22 [-24.636553] 0.0\n",
      "-10.253853\n",
      "1600 -16.9838493231361 19.18 [-20.807579] 0.0\n",
      "-9.989333\n",
      "1700 -18.69187813318539 20.82 [-25.433167] 0.0\n",
      "-10.960646\n",
      "1800 -16.733657010338135 19.0 [-22.52777] 0.0\n",
      "-8.088861\n",
      "1900 -17.05846273724878 19.29 [-21.489435] 0.0\n",
      "-12.164531\n",
      "2000 -17.8236960012795 20.09 [-22.47847] 0.0\n",
      "-11.180266\n",
      "2100 -16.97582323731734 19.15 [-21.736994] 0.0\n",
      "-11.251247\n",
      "2200 -16.652217808409798 18.82 [-20.118742] 0.0\n",
      "-11.29411\n",
      "2300 -14.691037500174529 17.12 [-18.056387] 0.0\n",
      "-11.519503\n",
      "2400 -15.470873355437359 17.66 [-20.681154] 0.0\n",
      "2.966159\n",
      "2500 -13.656026706273572 15.9 [-15.21733] 0.0\n",
      "-9.707323\n",
      "2600 -14.259834095399112 16.57 [-16.191132] 0.0\n",
      "-9.858579\n",
      "2700 -16.037177148254468 18.13 [-21.0229] 0.0\n",
      "-12.472367\n",
      "2800 -14.517846810002341 16.68 [-17.97462] 0.0\n",
      "-10.793636\n",
      "2900 -14.181114913916215 16.57 [-17.783993] 0.0\n",
      "-10.964391\n",
      "3000 -19.830567955721282 21.92 [-29.135454] 0.0\n",
      "-7.337845\n",
      "3100 -15.907584809205236 18.13 [-20.258373] 0.0\n",
      "-11.076667\n",
      "3200 -16.704525692882015 18.95 [-21.15772] 0.0\n",
      "-11.123268\n",
      "3300 -16.87160321988762 19.08 [-21.78924] 0.0\n",
      "-12.9408655\n",
      "3400 -17.03231072514067 19.43 [-20.333809] 0.0\n",
      "-11.064449\n",
      "3500 -14.828736894835515 17.17 [-17.58478] 0.0\n",
      "2.966159\n",
      "3600 -15.00906547274893 17.27 [-17.118725] 0.0\n",
      "-9.522196\n",
      "3700 -17.865536066670526 20.0 [-21.133623] 0.0\n",
      "6.9325113\n",
      "3800 -16.091999237051176 18.3 [-18.678396] 0.0\n",
      "-12.368722\n",
      "3900 -15.414685722966512 17.29 [-16.543852] 0.0\n",
      "-14.868206\n",
      "4000 -16.224003917062145 18.45 [-17.092354] 0.0\n",
      "-15.486891\n",
      "4100 -16.36833061209386 18.66 [-19.164198] 0.0\n",
      "-10.725937\n",
      "4200 -16.50237241534122 18.66 [-19.038198] 0.0\n",
      "-11.578716\n",
      "4300 -17.253431417076975 19.44 [-17.697197] 0.0\n",
      "-10.890276\n",
      "4400 -17.897703340423053 20.06 [-19.587605] 0.0\n",
      "-9.908015\n",
      "4500 -17.092656382149766 19.28 [-19.788574] 0.0\n",
      "-10.93086\n",
      "4600 -18.90462590558013 21.13 [-22.168707] 0.0\n",
      "-12.420229\n",
      "4700 -16.349374701742075 18.7 [-17.002804] 0.0\n",
      "-11.376289\n",
      "4800 -16.48072653953971 18.85 [-20.050724] 0.0\n",
      "-12.336738\n",
      "4900 -17.49884228677551 19.8 [-18.971176] 0.0\n",
      "-0.6157119\n",
      "5000 -15.947265601490788 18.13 [-18.367794] 0.0\n",
      "-10.133814\n",
      "5100 -16.300504209721012 18.62 [-17.854809] 0.0\n",
      "-11.193764\n",
      "5200 -14.868935350730087 17.16 [-16.251072] 0.0\n",
      "-12.084734\n",
      "5300 -16.11964683311746 18.31 [-17.992851] 0.0\n",
      "-12.191138\n",
      "5400 -15.608534049594766 17.86 [-15.648303] 0.0\n",
      "-11.464267\n",
      "5500 -17.057954742129272 19.32 [-18.06575] 0.0\n",
      "-12.227732\n",
      "5600 -16.392912390176853 18.65 [-17.407516] 0.0\n",
      "-12.563599\n",
      "5700 -15.31383890685797 17.67 [-16.159487] 0.0\n",
      "-10.449105\n",
      "5800 -14.917425676359128 17.05 [-16.044899] 0.0\n",
      "-10.386345\n",
      "5900 -15.229114465164335 17.59 [-13.378188] 0.0\n",
      "-13.266057\n",
      "6000 -17.439884762673604 19.71 [-19.035862] 0.0\n",
      "-9.857192\n",
      "6100 -16.501093812169504 18.56 [-18.009203] 0.0\n",
      "-9.730905\n",
      "6200 -13.72823490316802 16.2 [-13.124255] 0.0\n",
      "-12.309154\n",
      "6300 -17.62539607078591 19.82 [-17.799335] 0.0\n",
      "-16.210556\n",
      "6400 -16.967869050327728 19.12 [-16.678883] 0.0\n",
      "-11.8147745\n",
      "6500 -16.395292326690356 18.48 [-17.230938] 0.0\n",
      "-11.449896\n",
      "6600 -18.13399852027916 20.19 [-19.376688] 0.0\n",
      "-9.010534\n",
      "6700 -17.75773437670122 19.99 [-18.771168] 0.0\n",
      "-11.562293\n",
      "6800 -15.63970201697734 17.92 [-17.44327] 0.0\n",
      "-13.314715\n",
      "6900 -16.400144255130076 18.54 [-19.427923] 0.0\n",
      "-5.1380587\n",
      "7000 -16.451330664368065 18.69 [-19.153072] 0.0\n",
      "-13.9880295\n",
      "7100 -16.24922769614524 18.39 [-17.031933] 0.0\n",
      "-10.156918\n",
      "7200 -18.01065219032379 20.11 [-19.141682] 0.0\n",
      "-7.7959747\n",
      "7300 -17.042201180641314 19.38 [-18.260765] 0.0\n",
      "-10.26387\n",
      "7400 -17.255226766616296 19.56 [-17.644405] 0.0\n",
      "-11.638291\n",
      "7500 -17.544899181957973 19.95 [-18.485546] 0.0\n",
      "-9.659827\n",
      "7600 -14.107471928327373 16.66 [-14.543566] 0.0\n",
      "-11.693238\n",
      "7700 -17.45778771691258 19.69 [-19.007004] 0.0\n",
      "-10.214014\n",
      "7800 -15.48856956316736 17.95 [-16.808977] 0.0\n",
      "-10.501764\n",
      "7900 -14.822026750029181 17.15 [-14.973533] 0.0\n",
      "-11.336341\n",
      "8000 -14.727738422735847 17.07 [-13.700914] 0.0\n",
      "-11.371673\n",
      "8100 -13.6922459363595 16.11 [-13.534479] 0.0\n",
      "-10.434234\n",
      "8200 -14.75522576559451 17.17 [-13.921006] 0.0\n",
      "-11.940213\n",
      "8300 -14.635071760950206 16.85 [-12.941294] 0.0\n",
      "-10.429921\n",
      "8400 -16.170484623148823 18.53 [-16.807076] 0.0\n",
      "-11.38467\n",
      "8500 -15.469758191863468 17.66 [-17.462126] 0.0\n",
      "-10.401547\n",
      "8600 -15.308195546164773 17.53 [-15.137966] 0.0\n",
      "-10.687437\n",
      "8700 -14.937953062799403 17.25 [-15.101167] 0.0\n",
      "-11.5467205\n",
      "8800 -15.540728857876452 17.87 [-15.681749] 0.0\n",
      "-13.13346\n",
      "8900 -16.751520252141074 18.83 [-18.880022] 0.0\n",
      "-11.266985\n",
      "9000 -18.81334368125124 20.91 [-22.325012] 0.0\n",
      "-11.119951\n",
      "9100 -15.750268556443269 18.14 [-18.355791] 0.0\n",
      "-11.056529\n",
      "9200 -17.273725538087763 19.51 [-18.080925] 0.0\n",
      "-12.188653\n",
      "9300 -17.470095788292703 19.55 [-21.224329] 0.0\n",
      "-9.559551\n",
      "9400 -16.063003615864446 18.37 [-18.181799] 0.0\n",
      "-11.582295\n",
      "9500 -16.364905112448206 18.6 [-19.603] 0.0\n",
      "-6.491474\n",
      "9600 -20.477355360468522 22.35 [-24.791964] 0.0\n",
      "-3.8191159\n",
      "9700 -14.697639389072421 17.09 [-15.860696] 0.0\n",
      "-12.051026\n",
      "9800 -15.905044777282413 18.1 [-19.210001] 0.0\n",
      "-11.478177\n",
      "9900 -17.40117560117678 19.56 [-20.631313] 0.0\n",
      "-11.339501\n",
      "10000 -15.938523262384287 18.24 [-18.26279] 0.0\n",
      "-12.308913\n",
      "10100 -16.11309725840617 18.34 [-19.362003] 0.0\n",
      "-12.871879\n",
      "10200 -18.36505881444008 20.32 [-24.131441] 0.0\n",
      "-11.086943\n",
      "10300 -17.942297863461913 20.09 [-21.39024] 0.0\n",
      "-13.351626\n",
      "10400 -15.558174943099294 17.73 [-17.813444] 0.0\n",
      "-11.837072\n",
      "10500 -15.484738225407868 17.71 [-17.892807] 0.0\n",
      "-11.257228\n",
      "10600 -15.070452205127994 17.59 [-16.2635] 0.0\n",
      "-11.218481\n",
      "10700 -17.377661179010037 19.71 [-19.929022] 0.0\n",
      "-9.474849\n",
      "10800 -17.608195415129927 19.92 [-20.760914] 0.0\n",
      "-11.757619\n",
      "10900 -14.878000002596298 17.0 [-17.195711] 0.0\n",
      "-0.4143827\n",
      "11000 -13.208434152923232 15.59 [-13.742383] 0.0\n",
      "-10.500368\n",
      "11100 -14.687056106630127 17.04 [-15.593475] 0.0\n",
      "-12.834622\n",
      "11200 -14.403073271698215 16.95 [-13.459448] 0.0\n",
      "-12.243424\n",
      "11300 -18.69966044906765 20.79 [-20.839153] 0.0\n",
      "-13.278364\n",
      "11400 -16.248920812335996 18.49 [-16.911627] 0.0\n",
      "-11.904254\n",
      "11500 -11.993120339622083 14.53 [-11.566737] 0.0\n",
      "-11.617889\n",
      "11600 -14.278552630563368 16.48 [-14.928269] 0.0\n",
      "-12.06371\n",
      "11700 -16.27477194863151 18.71 [-15.294382] 0.0\n",
      "-10.195927\n",
      "11800 -16.043857422761636 18.37 [-15.619063] 0.0\n",
      "-5.9433355\n",
      "11900 -17.698306728250916 19.86 [-17.343683] 0.0\n",
      "-10.551805\n",
      "12000 -17.856396540631287 20.12 [-18.81818] 0.0\n",
      "-12.3990555\n",
      "12100 -17.11127583950361 19.32 [-16.472883] 0.0\n",
      "-11.075225\n",
      "12200 -19.116112717159115 21.23 [-20.317463] 0.0\n",
      "-11.721173\n",
      "12300 -16.531779514184954 18.93 [-15.886158] 0.0\n",
      "-11.082763\n",
      "12400 -16.187476455671653 18.3 [-17.253464] 0.0\n",
      "-11.2024145\n",
      "12500 -17.45395863063571 19.76 [-17.703897] 0.0\n",
      "-12.054685\n",
      "12600 -14.0713842475364 16.63 [-12.482715] 0.0\n",
      "-12.1051655\n",
      "12700 -15.807277568208221 18.09 [-17.13357] 0.0\n",
      "-9.909957\n",
      "12800 -14.84748819710724 17.17 [-15.6683445] 0.0\n",
      "-8.842075\n",
      "12900 -18.12302204213066 20.15 [-18.25366] 0.0\n",
      "-11.131611\n",
      "13000 -16.28130138318154 18.49 [-17.431147] 0.0\n",
      "-12.194088\n",
      "13100 -16.11767594717353 18.43 [-15.338614] 0.0\n",
      "-11.32492\n",
      "13200 -15.813006023349521 17.97 [-15.743738] 0.0\n",
      "8.682378\n",
      "13300 -15.613821209629416 17.79 [-16.717415] 0.0\n",
      "-9.090423\n",
      "13400 -15.569321353160593 17.89 [-16.507845] 0.0\n",
      "-11.425586\n",
      "13500 -15.92768717375727 18.21 [-16.008493] 0.0\n",
      "-12.739518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13600 -16.622623481859165 19.07 [-14.697456] 0.0\n",
      "2.966159\n",
      "13700 -17.691776194344087 19.84 [-17.570442] 0.0\n",
      "-10.423825\n",
      "13800 -15.776073772806331 18.28 [-13.231083] 0.0\n",
      "-13.123126\n",
      "13900 -18.106649935657575 20.34 [-16.396448] 0.0\n",
      "-10.716504\n",
      "14000 -15.988977958173512 18.2 [-14.900857] 0.0\n",
      "-12.324952\n",
      "14100 -17.46282232515978 19.91 [-15.543653] 0.0\n",
      "-15.090933\n",
      "14200 -17.956457032654974 20.4 [-19.330502] 0.0\n",
      "-11.176481\n",
      "14300 -17.69001655705851 19.93 [-19.290087] 0.0\n",
      "5.0958595\n",
      "14400 -17.403428926798384 19.47 [-18.423807] 0.0\n",
      "-9.939711\n",
      "14500 -15.72096457704061 17.96 [-16.292032] 0.0\n",
      "-10.641548\n",
      "14600 -16.52638816155309 18.98 [-17.72571] 0.0\n",
      "-13.197895\n",
      "14700 -16.87868053818299 19.18 [-15.250724] 0.0\n",
      "2.966159\n",
      "14800 -16.629055306684116 18.79 [-16.901888] 0.0\n",
      "-11.44282\n",
      "14900 -18.846030490627264 21.13 [-17.152128] 0.0\n",
      "-9.480846\n",
      "15000 -16.857317222449144 19.01 [-17.52759] 0.0\n",
      "-9.9079075\n",
      "15100 -17.486530513053303 19.69 [-20.169762] 0.0\n",
      "-11.238624\n",
      "15200 -16.276382149764203 18.7 [-17.822481] 0.0\n",
      "-11.282498\n",
      "15300 -15.462807237993902 17.81 [-14.309188] 0.0\n",
      "-11.44418\n",
      "15400 -16.07623887096382 18.36 [-14.674509] 0.0\n",
      "-10.823342\n",
      "15500 -16.263706225420762 18.7 [-14.365369] 0.0\n",
      "-11.227629\n",
      "15600 -16.79573650431456 18.99 [-16.162773] 0.0\n",
      "-11.705741\n",
      "15700 -17.820807004086348 20.02 [-16.162859] 0.0\n",
      "-12.501051\n",
      "15800 -17.98952412788611 20.27 [-16.095436] 0.0\n",
      "-11.71342\n",
      "15900 -17.86714939055643 20.02 [-16.573706] 0.0\n",
      "-11.591599\n",
      "16000 -18.008277944975962 20.31 [-16.969929] 0.0\n",
      "-13.254775\n",
      "16100 -14.212211267181619 16.72 [-12.037019] 0.0\n",
      "-12.452308\n",
      "16200 -16.983822282277078 19.4 [-13.497723] 0.0\n",
      "-11.481324\n",
      "16300 -17.413132699386537 19.67 [-15.581357] 0.0\n",
      "-11.6350975\n",
      "16400 -15.930262931108123 18.27 [-13.750943] 0.0\n",
      "-13.320646\n",
      "16500 -15.47980535527348 17.86 [-14.163963] 0.0\n",
      "-12.023931\n",
      "16600 -16.355686906191266 18.77 [-15.048126] 0.0\n",
      "-2.3535678\n",
      "16700 -16.04982710083696 18.42 [-15.972578] 0.0\n",
      "-13.22016\n",
      "16800 -16.08711870878007 18.44 [-15.865971] 0.0\n",
      "-12.759763\n",
      "16900 -17.592211431347334 19.89 [-15.965425] 0.0\n",
      "-4.325919\n",
      "17000 -17.57867043975214 19.83 [-16.716022] 0.0\n",
      "-11.365697\n",
      "17100 -15.376553346760351 17.78 [-14.318844] 0.0\n",
      "-11.126989\n",
      "17200 -17.17978846845403 19.36 [-18.31508] 0.0\n",
      "-9.908202\n",
      "17300 -15.976280006683673 18.46 [-15.306051] 0.0\n",
      "-13.257263\n",
      "17400 -17.45724220788549 19.82 [-17.36273] 0.0\n",
      "-9.749984\n",
      "17500 -18.267516886028826 20.44 [-17.338272] 0.0\n",
      "-7.7522564\n",
      "17600 -14.900798084291786 17.25 [-14.925492] 0.0\n",
      "-13.247441\n",
      "17700 -20.93223871477587 23.07 [-22.061928] 0.0\n",
      "-12.491344\n",
      "17800 -20.09765382150886 22.19 [-20.83443] 0.0\n",
      "-11.7801695\n",
      "17900 -14.187212650262063 16.57 [-13.752629] 0.0\n",
      "-10.010459\n",
      "18000 -16.499880155402856 18.65 [-17.566898] 0.0\n",
      "-10.406491\n",
      "18100 -17.01715124273315 19.34 [-17.738018] 0.0\n",
      "-10.200049\n",
      "18200 -17.55112067482829 19.82 [-18.041958] 0.0\n",
      "-12.114601\n",
      "18300 -18.805280621788278 21.01 [-19.223282] 0.0\n",
      "-10.60168\n",
      "18400 -17.80021407598609 20.0 [-17.439028] 0.0\n",
      "-12.183379\n",
      "18500 -14.60098318388362 17.11 [-14.748445] 0.0\n",
      "-9.660862\n",
      "18600 -17.027544502481252 19.31 [-17.823874] 0.0\n",
      "-4.2885036\n",
      "18700 -13.592422516615603 15.98 [-14.358594] 0.0\n",
      "-12.975178\n",
      "18800 -14.55824741470772 16.93 [-15.033197] 0.0\n",
      "-12.528337\n",
      "18900 -17.189188183305966 19.49 [-18.986353] 0.0\n",
      "-12.100858\n",
      "19000 -18.313044199407294 20.53 [-18.44662] 0.0\n",
      "6.9325113\n",
      "19100 -15.819301185368667 18.24 [-17.448233] 0.0\n",
      "-11.1467495\n",
      "19200 -16.609961794850104 18.83 [-19.961237] 0.0\n",
      "-10.87743\n",
      "19300 -18.35232826036131 20.68 [-19.898462] 0.0\n",
      "-13.741841\n",
      "19400 -17.91395765358643 20.11 [-19.775457] 0.0\n",
      "-11.039001\n",
      "19500 -18.376059982463634 20.57 [-19.183805] 0.0\n",
      "-9.029713\n",
      "19600 -16.404493537883006 18.71 [-17.077974] 0.0\n",
      "-12.018979\n",
      "19700 -15.792987488396669 18.2 [-16.896843] 0.0\n",
      "-10.874591\n",
      "19800 -18.88684261301913 21.19 [-21.949177] 0.0\n",
      "-6.085635\n",
      "19900 -16.57309543590421 18.81 [-19.19593] 0.0\n",
      "-11.509289\n",
      "20000 -18.35455826731217 20.45 [-20.440094] 0.0\n",
      "-10.424015\n",
      "20100 -18.79643109576257 20.94 [-23.067127] 0.0\n",
      "2.5731847\n",
      "20200 -17.63410127559363 19.85 [-20.288795] 0.0\n",
      "-12.763632\n",
      "20300 -17.88428427847556 20.1 [-20.280066] 0.0\n",
      "-11.955137\n",
      "20400 -18.02180893718547 20.2 [-20.449587] 0.0\n",
      "-11.479749\n",
      "20500 -18.319885686983394 20.61 [-21.512863] 0.0\n",
      "-12.106302\n",
      "20600 -18.749318223460545 21.04 [-21.399656] 0.0\n",
      "-10.413053\n",
      "20700 -15.055020610201693 17.41 [-16.573137] 0.0\n",
      "-12.302167\n",
      "20800 -17.917353579298563 20.13 [-21.287651] 0.0\n",
      "-10.692948\n",
      "20900 -18.389297325659502 20.58 [-21.092453] 0.0\n",
      "-12.355426\n",
      "21000 -17.51559995534995 19.73 [-18.93058] 0.0\n",
      "-9.908612\n",
      "21100 -14.045976248701729 16.31 [-16.466606] 0.0\n",
      "-10.620273\n",
      "21200 -16.095978378835902 18.58 [-17.145529] 0.0\n",
      "-8.878822\n",
      "21300 -17.475745056925394 19.79 [-18.659645] 0.0\n",
      "-11.766579\n",
      "21400 -18.596814480073796 20.76 [-19.324999] 0.0\n",
      "-11.131729\n",
      "21500 -17.433356782432785 19.77 [-16.396687] 0.0\n",
      "-11.927966\n",
      "21600 -18.791707703298286 20.89 [-19.052132] 0.0\n",
      "-11.010397\n",
      "21700 -16.077491032454788 18.47 [-17.09095] 0.0\n",
      "-11.472076\n",
      "21800 -18.661996062391022 20.95 [-19.413292] 0.0\n",
      "-8.880002\n",
      "21900 -16.79534554887161 19.11 [-18.680977] 0.0\n",
      "-11.324981\n",
      "22000 -18.389223789395952 20.6 [-20.297052] 0.0\n",
      "-10.79803\n",
      "22100 -17.905619131308924 20.13 [-20.534397] 0.0\n",
      "-13.369317\n",
      "22200 -18.464984449367577 20.65 [-18.70695] 0.0\n",
      "-11.633943\n",
      "22300 -16.23002225926784 18.55 [-18.344786] 0.0\n",
      "-3.7533948\n",
      "22400 -18.188248388836527 20.46 [-19.182852] 0.0\n",
      "-10.81436\n",
      "22500 -15.795958298676387 18.22 [-18.150362] 0.0\n",
      "-12.851352\n",
      "22600 -17.763892483754493 19.89 [-17.806814] 0.0\n",
      "-13.058702\n",
      "22700 -18.52235565260679 20.83 [-18.774437] 0.0\n",
      "8.682378\n",
      "22800 -15.490393050722185 17.91 [-15.167685] 0.0\n",
      "-1.8049324\n",
      "22900 -15.102688790840478 17.45 [-14.183358] 0.0\n",
      "-11.35158\n",
      "23000 -16.25532839500209 18.51 [-13.949681] 0.0\n",
      "-6.491474\n",
      "23100 -15.407915194051293 17.83 [-12.968963] 0.0\n",
      "-11.633788\n",
      "23200 -15.06529105339997 17.31 [-12.09889] 0.0\n",
      "3.6212418\n",
      "23300 -15.853090553696585 18.16 [-15.155874] 0.0\n",
      "3.6212418\n",
      "23400 -16.966824685021876 19.23 [-14.144912] 0.0\n",
      "-14.077434\n",
      "23500 -17.433536941042583 19.59 [-16.152315] 0.0\n",
      "-7.542095\n",
      "23600 -16.397760034681205 18.67 [-15.435081] 0.0\n",
      "-12.867037\n",
      "23700 -15.726668575522474 18.26 [-14.809174] 0.0\n",
      "-9.334156\n",
      "23800 -16.193605042326503 18.52 [-13.2437315] 0.0\n",
      "-11.75014\n",
      "23900 -19.485323543023796 21.64 [-20.535847] 0.0\n",
      "-11.82389\n",
      "24000 -14.176706278315656 16.5 [-14.041747] 0.0\n",
      "-11.542181\n",
      "24100 -17.33089035291619 19.81 [-16.607286] 0.0\n",
      "-12.242875\n",
      "24200 -15.66013475198221 18.05 [-13.741263] 0.0\n",
      "-13.324251\n",
      "24300 -17.472067463424583 19.64 [-14.82054] 0.0\n",
      "-11.391604\n",
      "24400 -17.635588750070482 19.89 [-15.639758] 0.0\n",
      "-9.294543\n",
      "24500 -17.09096181664341 19.4 [-16.273623] 0.0\n",
      "-10.884706\n",
      "24600 -17.642467539592165 19.75 [-14.746671] 0.0\n",
      "-13.881909\n",
      "24700 -17.144892783174697 19.25 [-16.914663] 0.0\n",
      "-8.263579\n",
      "24800 -14.654872036493998 17.01 [-13.726] 0.0\n",
      "-8.467552\n",
      "24900 -15.184430369559907 17.73 [-14.191407] 0.0\n",
      "-8.069528\n",
      "25000 -17.379192653583253 19.67 [-16.823973] 0.0\n",
      "-4.405942\n",
      "25100 -16.73122037280591 19.03 [-14.884374] 0.0\n",
      "-9.724064\n",
      "25200 -17.292259538769343 19.56 [-16.249475] 0.0\n",
      "-10.408575\n",
      "25300 -19.007456343395848 21.28 [-17.68347] 0.0\n",
      "-11.367297\n",
      "25400 -14.941086897033863 17.24 [-12.613894] 0.0\n",
      "-10.30865\n",
      "25500 -18.258649009537994 20.53 [-15.433264] 0.0\n",
      "-14.81475\n",
      "25600 -17.4473844624829 19.68 [-14.3881035] 0.0\n",
      "-13.165116\n",
      "25700 -18.362448065358862 20.58 [-17.215843] 0.0\n",
      "-10.141646\n",
      "25800 -15.161555148279067 17.55 [-12.404393] 0.0\n",
      "-11.143228\n",
      "25900 -17.690888837867814 19.7 [-17.757425] 0.0\n",
      "-12.293951\n",
      "26000 -14.874381027391363 17.32 [-14.863723] 0.0\n",
      "-11.4764185\n",
      "26100 -16.61595177870287 18.78 [-14.366145] 0.0\n",
      "-10.810189\n",
      "26200 -16.446299928227884 18.64 [-15.25879] 0.0\n",
      "-5.4662724\n",
      "26300 -16.6173782421663 18.89 [-16.865206] 0.0\n",
      "-11.388413\n",
      "26400 -17.944805542886755 20.18 [-17.25832] 0.0\n",
      "-14.300992\n",
      "26500 -16.59606054769336 18.79 [-14.515779] 0.0\n",
      "-12.181375\n",
      "26600 -18.157469631399298 20.31 [-16.377924] 0.0\n",
      "-12.096671\n",
      "26700 -16.395875171583345 18.63 [-15.461503] 0.0\n",
      "-13.20404\n",
      "26800 -16.937871993138362 19.22 [-15.146459] 0.0\n",
      "-9.647153\n",
      "26900 -18.30231107155366 20.54 [-17.98276] 0.0\n",
      "-11.397878\n",
      "27000 -17.47463335246204 19.81 [-14.685221] 0.0\n",
      "-5.0683527\n",
      "27100 -17.65332199018683 19.73 [-15.995553] 0.0\n",
      "-12.0441885\n",
      "27200 -16.841734597231405 18.95 [-15.341162] 0.0\n",
      "-9.037419\n",
      "27300 -16.62474111497783 18.94 [-13.739871] 0.0\n",
      "-12.289232\n",
      "27400 -17.2080639855237 19.22 [-13.497566] 0.0\n",
      "-14.072155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27500 -16.674450172864464 18.93 [-14.477551] 0.0\n",
      "-11.856477\n",
      "27600 -16.530717280697843 18.64 [-15.792402] 0.0\n",
      "-10.794404\n",
      "27700 -17.496727756116975 19.68 [-15.864621] 0.0\n",
      "-11.372762\n",
      "27800 -17.33832983282263 19.57 [-15.916283] 0.0\n",
      "-11.819462\n",
      "27900 -18.857654095310743 21.14 [-16.966642] 0.0\n",
      "-12.455564\n",
      "28000 -16.06393805082548 18.43 [-13.609148] 0.0\n",
      "4.331053\n",
      "28100 -16.108411769626233 18.37 [-11.895208] 0.0\n",
      "-12.145089\n",
      "28200 -16.638091838716786 18.91 [-16.539833] 0.0\n",
      "-1.8996799\n",
      "28300 -18.002295042748443 20.4 [-16.720274] 0.0\n",
      "-9.257013\n",
      "28400 -16.644914930350012 18.95 [-16.352503] 0.0\n",
      "-11.182559\n",
      "28500 -17.731119372073056 19.83 [-17.806324] 0.0\n",
      "-8.950923\n",
      "28600 -19.860399589568686 21.69 [-18.4844] 0.0\n",
      "-11.001533\n",
      "28700 -16.707307186500586 18.94 [-15.957067] 0.0\n",
      "-11.424487\n",
      "28800 -14.968315857398395 17.29 [-13.162794] 0.0\n",
      "-10.163217\n",
      "28900 -17.216688011594364 19.34 [-13.710572] 0.0\n",
      "-10.292967\n",
      "Testing file 0\n",
      "Testing file 100\n",
      "Testing file 200\n",
      "Testing file 300\n",
      "Testing file 400\n",
      "Testing file 500\n",
      "Testing file 600\n",
      "Testing file 700\n",
      "Testing file 800\n",
      "Testing file 900\n",
      "-16.37323110006873 18.628\n",
      "[5424 1813  528 9138  682 1043]\n",
      "Done testing in 30.8 s\n",
      "\n",
      "Testing file 0\n",
      "Testing file 100\n",
      "Testing file 200\n",
      "Testing file 300\n",
      "Testing file 400\n",
      "Testing file 500\n",
      "Testing file 600\n",
      "Testing file 700\n",
      "Testing file 800\n",
      "Testing file 900\n",
      "[    0 16310     0     0     1]\n",
      "-14.07849091855865 16.311\n",
      "Done testing random policy in  47.51 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actions = 6   \n",
    "state_space = 4\n",
    "\n",
    "directory = '../Tests/CNFGEN_20/'  # RLSAT problems are very simple. SATLIB probelms give more interesting Q-values.\n",
    "files = list(map(lambda x: os.path.join(directory, x), os.listdir(directory)))\n",
    "\n",
    "split = int(len(files) * 0.3)\n",
    "training_files = files[:split]\n",
    "test_files = files[60000:61000]\n",
    "\n",
    "print(\"Number of training files:\", len(training_files))\n",
    "print(\"Number of test files:\", len(test_files))\n",
    "\n",
    "episode_reward_train, episode_length_train, losses, estimator = actor_critic(training_files, output_stats_every=100)\n",
    "\n",
    "s = time.time()\n",
    "episode_reward_test, episode_length_test, episode_actions = test(test_files, ϵ=0, estimator=estimator)\n",
    "print(episode_reward_test, episode_length_test)\n",
    "print(np.bincount(episode_actions))\n",
    "e = time.time()\n",
    "print(\"Done testing in\", (round(e-s, 2)), \"s\")\n",
    "print()\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "episode_reward_rand, episode_length_rand, episode_actions_rand = test(test_files)\n",
    "print(np.bincount(episode_actions_rand))\n",
    "print(episode_reward_rand, episode_length_rand)\n",
    "e = time.time()\n",
    "print(\"Done testing random policy in \", (round(e-s, 2)), \"s\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %debug\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Average reward')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "episode_reward_train = np.array(episode_reward_train).reshape((-1, 10))\n",
    "episode_reward_train = np.mean(episode_reward_train, axis=1).squeeze()\n",
    "\n",
    "plt.plot(episode_reward_train[1:], label='Training')\n",
    "\n",
    "plt.xlabel(\"Number of episodes (1000s)\")\n",
    "plt.ylabel(\"Average reward\")\n",
    "\n",
    "# tikz_save('basic_reward_plus_frac_clause.tikz', figureheight='\\\\figureheight', figurewidth='\\\\figurewidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_length_train[1:], label='Training')\n",
    "\n",
    "plt.xlabel(\"Number of episodes (1000s)\")\n",
    "plt.ylabel(\"Average reward\")\n",
    "\n",
    "# tikz_save('basic_reward_plus_frac_clause.tikz', figureheight='\\\\figureheight', figurewidth='\\\\figurewidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[1:], label='Training')\n",
    "\n",
    "plt.xlabel(\"Number of episodes (1000s)\")\n",
    "plt.ylabel(\"Average reward\")\n",
    "\n",
    "# tikz_save('basic_reward_plus_frac_clause.tikz', figureheight='\\\\figureheight', figurewidth='\\\\figurewidth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
